#!/usr/bin/env python3
"""
KubeSphereå¼€å‘æŒ‡å—çˆ¬è™«
æŠ“å–https://dev-guide.kubesphere.io/extension-dev-guide/zh/çš„æ‰€æœ‰å†…å®¹
"""

import requests
from bs4 import BeautifulSoup
import time
import os
from urllib.parse import urljoin, urlparse
from pathlib import Path
import json
import html2text
from reportlab.lib.pagesizes import letter
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.lib.enums import TA_LEFT
from reportlab.pdfbase import pdfmetrics
from reportlab.pdfbase.ttfonts import TTFont
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

class KubeSphereDocCrawler:
    def __init__(self, base_url="https://dev-guide.kubesphere.io/extension-dev-guide/zh/"):
        self.base_url = base_url
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        })
        self.visited_urls = set()
        self.content_data = []
        self.h2md = html2text.HTML2Text()
        self.h2md.ignore_links = False
        self.h2md.ignore_images = False
        
        # ç¡®ä¿dataç›®å½•å­˜åœ¨
        self.data_dir = Path("data")
        self.data_dir.mkdir(exist_ok=True)
    
    def fetch_page(self, url):
        """è·å–é¡µé¢å†…å®¹"""
        try:
            response = self.session.get(url, timeout=10)
            if response.status_code == 404:
                logging.warning(f"é¡µé¢ä¸å­˜åœ¨ {url}")
                return None
            response.raise_for_status()
            return response.text
        except requests.RequestException as e:
            logging.error(f"è·å–é¡µé¢å¤±è´¥ {url}: {e}")
            return None
    
    def extract_navigation_links(self, html_content, base_url):
        """æå–å¯¼èˆªèœå•ä¸­çš„æ‰€æœ‰é“¾æ¥"""
        soup = BeautifulSoup(html_content, 'html.parser')
        links = set()
        
        # é¢„å®šä¹‰çš„ä¸»è¦ç« èŠ‚è·¯å¾„
        main_sections = [
            "/extension-dev-guide/zh/overview/",
            "/extension-dev-guide/zh/quickstart/", 
            "/extension-dev-guide/zh/feature-customization/",
            "/extension-dev-guide/zh/examples/",
            "/extension-dev-guide/zh/packaging-and-release/",
            "/extension-dev-guide/zh/best-practices/",
            "/extension-dev-guide/zh/FAQ/",
            "/extension-dev-guide/zh/migration/",
            "/extension-dev-guide/zh/references/"
        ]
        
        # æ·»åŠ é¢„å®šä¹‰çš„ä¸»è¦ç« èŠ‚
        for section in main_sections:
            full_url = urljoin(base_url, section)
            links.add(full_url)
        
        # æŸ¥æ‰¾é¡µé¢ä¸­çš„æ‰€æœ‰å†…éƒ¨é“¾æ¥
        for link in soup.find_all('a', href=True):
            href = link['href']
            if href.startswith('/extension-dev-guide/zh/'):
                full_url = urljoin(base_url, href)
                links.add(full_url)
            elif href.startswith('http') and 'dev-guide.kubesphere.io/extension-dev-guide/zh/' in href:
                links.add(href)
        
        # é¢å¤–æ£€æŸ¥å¯èƒ½çš„å­é¡µé¢
        subsection_patterns = [
            "overview", "quickstart", "feature-customization", "examples", 
            "packaging-and-release", "best-practices", "FAQ", "migration", "references"
        ]
        
        for pattern in subsection_patterns:
            # æ£€æŸ¥æ¯ä¸ªä¸»è¦ç« èŠ‚çš„å¯èƒ½å­é¡µé¢
            potential_urls = [
                f"/extension-dev-guide/zh/{pattern}/",
                f"/extension-dev-guide/zh/{pattern}/index.html",
                f"/extension-dev-guide/zh/{pattern}.html"
            ]
            for url_path in potential_urls:
                full_url = urljoin(base_url, url_path)
                links.add(full_url)
        
        return list(links)
    
    def extract_content(self, html_content, url):
        """æå–é¡µé¢ä¸»è¦å†…å®¹"""
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # ç§»é™¤å¯¼èˆªã€ä¾§è¾¹æ ã€è„šæ³¨ç­‰
        for element in soup.find_all(['nav', 'aside', 'footer', 'header']):
            element.decompose()
        
        # ç§»é™¤è„šæœ¬å’Œæ ·å¼
        for element in soup.find_all(['script', 'style']):
            element.decompose()
        
        # æŸ¥æ‰¾ä¸»è¦å†…å®¹åŒºåŸŸ
        main_content = (
            soup.find('main') or 
            soup.find('article') or 
            soup.find('div', class_=lambda x: x and 'content' in x.lower()) or
            soup.find('div', id=lambda x: x and 'content' in x.lower()) or
            soup.body
        )
        
        if not main_content:
            return None, None
        
        # æå–æ ‡é¢˜
        title_elem = soup.find('h1') or soup.find('title')
        title = title_elem.get_text().strip() if title_elem else urlparse(url).path.split('/')[-1]
        
        # è½¬æ¢ä¸ºmarkdown
        markdown_content = self.h2md.handle(str(main_content))
        
        return title, markdown_content
    
    def crawl_all_pages(self):
        """çˆ¬å–æ‰€æœ‰é¡µé¢"""
        logging.info("å¼€å§‹çˆ¬å–KubeSphereå¼€å‘æŒ‡å—...")
        
        # è·å–é¦–é¡µ
        html_content = self.fetch_page(self.base_url)
        if not html_content:
            logging.error("æ— æ³•è·å–é¦–é¡µå†…å®¹")
            return
        
        # æå–æ‰€æœ‰é“¾æ¥
        all_links = self.extract_navigation_links(html_content, self.base_url)
        all_links.append(self.base_url)  # åŒ…å«é¦–é¡µ
        
        logging.info(f"å‘ç° {len(all_links)} ä¸ªé¡µé¢é“¾æ¥")
        
        # çˆ¬å–æ¯ä¸ªé¡µé¢
        for i, url in enumerate(all_links, 1):
            if url in self.visited_urls:
                continue
                
            logging.info(f"æ­£åœ¨çˆ¬å– ({i}/{len(all_links)}): {url}")
            html_content = self.fetch_page(url)
            
            if html_content:
                title, content = self.extract_content(html_content, url)
                if title and content:
                    self.content_data.append({
                        'url': url,
                        'title': title,
                        'content': content
                    })
                    self.visited_urls.add(url)
            
            # ç¤¼è²Œç­‰å¾…
            time.sleep(1)
        
        logging.info(f"çˆ¬å–å®Œæˆï¼Œå…±è·å– {len(self.content_data)} ä¸ªé¡µé¢å†…å®¹")
    
    def save_as_markdown(self):
        """ä¿å­˜ä¸ºmarkdownæ–‡æ¡£"""
        output_path = self.data_dir / "kubesphere_dev_guide.md"
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write("# KubeSphereæ‰©å±•å¼€å‘æŒ‡å—\n\n")
            f.write("æœ¬æ–‡æ¡£æ¥æºï¼šhttps://dev-guide.kubesphere.io/extension-dev-guide/zh/\n\n")
            f.write("---\n\n")
            
            for page in self.content_data:
                f.write(f"# {page['title']}\n\n")
                f.write(f"æ¥æºï¼š{page['url']}\n\n")
                f.write(page['content'])
                f.write("\n\n---\n\n")
        
        logging.info(f"Markdownæ–‡æ¡£å·²ä¿å­˜åˆ°: {output_path}")
        return output_path
    
    def save_as_pdf(self):
        """ä¿å­˜ä¸ºPDFæ–‡æ¡£"""
        try:
            output_path = self.data_dir / "kubesphere_dev_guide.pdf"
            doc = SimpleDocTemplate(str(output_path), pagesize=letter)
            styles = getSampleStyleSheet()
            
            # åˆ›å»ºæ”¯æŒä¸­æ–‡çš„æ ·å¼
            title_style = ParagraphStyle(
                'ChineseTitle',
                parent=styles['Heading1'],
                fontName='Helvetica-Bold',
                fontSize=16,
                spaceAfter=12,
                alignment=TA_LEFT
            )
            
            body_style = ParagraphStyle(
                'ChineseBody',
                parent=styles['Normal'],
                fontName='Helvetica',
                fontSize=10,
                spaceAfter=6,
                alignment=TA_LEFT
            )
            
            story = []
            
            # æ·»åŠ æ–‡æ¡£æ ‡é¢˜
            story.append(Paragraph("KubeSphereæ‰©å±•å¼€å‘æŒ‡å—", title_style))
            story.append(Spacer(1, 12))
            
            for page in self.content_data:
                # é¡µé¢æ ‡é¢˜
                story.append(Paragraph(page['title'], title_style))
                story.append(Spacer(1, 6))
                
                # æ¥æºURL
                story.append(Paragraph(f"æ¥æºï¼š{page['url']}", body_style))
                story.append(Spacer(1, 6))
                
                # å†…å®¹ï¼ˆç®€åŒ–å¤„ç†ï¼Œå»æ‰markdownæ ¼å¼ï¼‰
                clean_content = page['content'].replace('#', '').replace('*', '').replace('`', '')
                paragraphs = clean_content.split('\n\n')
                
                for para in paragraphs[:20]:  # é™åˆ¶æ®µè½æ•°é‡é¿å…PDFè¿‡å¤§
                    if para.strip():
                        story.append(Paragraph(para.strip(), body_style))
                        story.append(Spacer(1, 3))
                
                story.append(Spacer(1, 12))
            
            doc.build(story)
            logging.info(f"PDFæ–‡æ¡£å·²ä¿å­˜åˆ°: {output_path}")
            return output_path
            
        except Exception as e:
            logging.error(f"PDFç”Ÿæˆå¤±è´¥: {e}")
            return None

def main():
    """ä¸»å‡½æ•°"""
    crawler = KubeSphereDocCrawler()
    
    # çˆ¬å–æ‰€æœ‰é¡µé¢
    crawler.crawl_all_pages()
    
    if not crawler.content_data:
        logging.error("æ²¡æœ‰è·å–åˆ°ä»»ä½•å†…å®¹")
        return
    
    # ä¿å­˜ä¸ºmarkdown
    md_path = crawler.save_as_markdown()
    
    # ä¿å­˜ä¸ºPDF
    pdf_path = crawler.save_as_pdf()
    
    print(f"\nçˆ¬å–å®Œæˆï¼")
    print(f"ğŸ“„ Markdownæ–‡æ¡£: {md_path}")
    if pdf_path:
        print(f"ğŸ“„ PDFæ–‡æ¡£: {pdf_path}")
    print(f"ğŸ“Š å…±çˆ¬å– {len(crawler.content_data)} ä¸ªé¡µé¢")

if __name__ == "__main__":
    main()