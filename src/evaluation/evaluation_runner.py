"""RAGè¯„ä¼°è¿è¡Œå™¨

æ•´åˆæ‰€æœ‰è¯„ä¼°ç»„ä»¶ï¼Œæä¾›ç®€å•çš„è¯„ä¼°æ¥å£å’ŒæŠ¥å‘Šç”ŸæˆåŠŸèƒ½ã€‚
"""

from typing import List, Dict, Any, Optional, Tuple
import time
import json
import os
from datetime import datetime
from loguru import logger

from src.generation.generator import RAGGenerator
from src.retrieval.retriever import HybridRetriever
from src.evaluation.rag_vs_baseline_evaluator import (
    RAGVsBaselineEvaluator, ComparisonResult, OverallEvaluation, EvaluationQuestion
)
from src.evaluation.kubesphere_test_questions import (
    KubeSphereQuestionBank, KubeSphereTestQuestion, get_quick_evaluation_questions
)


class EvaluationRunner:
    """RAGè¯„ä¼°è¿è¡Œå™¨"""

    def __init__(
        self,
        rag_generator: RAGGenerator,
        retriever: HybridRetriever,
        output_dir: str = "data/evaluation_results"
    ):
        """åˆå§‹åŒ–è¯„ä¼°è¿è¡Œå™¨

        Args:
            rag_generator: RAGç”Ÿæˆå™¨
            retriever: æ£€ç´¢å™¨
            output_dir: ç»“æœè¾“å‡ºç›®å½•
        """
        self.rag_generator = rag_generator
        self.retriever = retriever
        self.output_dir = output_dir

        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)

        # åˆå§‹åŒ–è¯„ä¼°å™¨
        self.evaluator = RAGVsBaselineEvaluator(rag_generator, retriever)

        logger.info(f"è¯„ä¼°è¿è¡Œå™¨åˆå§‹åŒ–å®Œæˆï¼Œç»“æœå°†ä¿å­˜è‡³: {output_dir}")

    def run_kubesphere_evaluation(
        self,
        question_set: str = "quick",
        save_results: bool = True
    ) -> Tuple[List[ComparisonResult], OverallEvaluation, str]:
        """è¿è¡ŒKubeSphereä¸“é—¨è¯„ä¼°

        Args:
            question_set: é—®é¢˜é›†ç±»å‹ ("quick", "full", "category:xxx", "difficulty:xxx")
            save_results: æ˜¯å¦ä¿å­˜ç»“æœ

        Returns:
            (è¯¦ç»†ç»“æœ, æ•´ä½“è¯„ä¼°, æŠ¥å‘Šæ–‡ä»¶è·¯å¾„)
        """
        logger.info(f"å¼€å§‹KubeSphereè¯„ä¼°ï¼Œé—®é¢˜é›†: {question_set}")

        # è·å–æµ‹è¯•é—®é¢˜
        kubesphere_questions = self._get_kubesphere_questions(question_set)
        logger.info(f"è·å–åˆ° {len(kubesphere_questions)} ä¸ªæµ‹è¯•é—®é¢˜")

        # è½¬æ¢ä¸ºè¯„ä¼°é—®é¢˜æ ¼å¼
        evaluation_questions = [
            EvaluationQuestion(
                question=kq.question,
                category=kq.category,
                expected_knowledge=kq.expected_knowledge,
                difficulty=kq.difficulty,
                ground_truth_answer=kq.ground_truth_answer
            )
            for kq in kubesphere_questions
        ]

        # è¿è¡Œè¯„ä¼°
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_file = None
        if save_results:
            results_file = os.path.join(
                self.output_dir, f"kubesphere_evaluation_{timestamp}.json"
            )

        detailed_results, overall_evaluation = self.evaluator.evaluate_question_set(
            evaluation_questions, save_results=save_results, results_file=results_file
        )

        # ç”Ÿæˆä¸“é—¨çš„KubeSphereæŠ¥å‘Š
        report_file = self._generate_kubesphere_report(
            detailed_results, overall_evaluation, timestamp
        )

        logger.info(f"KubeSphereè¯„ä¼°å®Œæˆï¼ŒæŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
        return detailed_results, overall_evaluation, report_file

    def run_custom_evaluation(
        self,
        questions: List[str],
        categories: Optional[List[str]] = None,
        difficulties: Optional[List[str]] = None,
        save_results: bool = True
    ) -> Tuple[List[ComparisonResult], OverallEvaluation, str]:
        """è¿è¡Œè‡ªå®šä¹‰é—®é¢˜è¯„ä¼°

        Args:
            questions: é—®é¢˜åˆ—è¡¨
            categories: å¯¹åº”çš„ç±»åˆ«åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
            difficulties: å¯¹åº”çš„éš¾åº¦åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
            save_results: æ˜¯å¦ä¿å­˜ç»“æœ

        Returns:
            (è¯¦ç»†ç»“æœ, æ•´ä½“è¯„ä¼°, æŠ¥å‘Šæ–‡ä»¶è·¯å¾„)
        """
        logger.info(f"å¼€å§‹è‡ªå®šä¹‰è¯„ä¼°ï¼Œå…± {len(questions)} ä¸ªé—®é¢˜")

        # æ„é€ è¯„ä¼°é—®é¢˜
        evaluation_questions = []
        for i, question in enumerate(questions):
            category = categories[i] if categories and i < len(categories) else "è‡ªå®šä¹‰"
            difficulty = difficulties[i] if difficulties and i < len(difficulties) else "ä¸­ç­‰"

            evaluation_questions.append(
                EvaluationQuestion(
                    question=question,
                    category=category,
                    expected_knowledge="ç”¨æˆ·è‡ªå®šä¹‰é—®é¢˜",
                    difficulty=difficulty,
                    ground_truth_answer=None
                )
            )

        # è¿è¡Œè¯„ä¼°
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_file = None
        if save_results:
            results_file = os.path.join(
                self.output_dir, f"custom_evaluation_{timestamp}.json"
            )

        detailed_results, overall_evaluation = self.evaluator.evaluate_question_set(
            evaluation_questions, save_results=save_results, results_file=results_file
        )

        # ç”ŸæˆæŠ¥å‘Š
        report_file = self._generate_custom_report(
            detailed_results, overall_evaluation, timestamp
        )

        logger.info(f"è‡ªå®šä¹‰è¯„ä¼°å®Œæˆï¼ŒæŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
        return detailed_results, overall_evaluation, report_file

    def _get_kubesphere_questions(self, question_set: str) -> List[KubeSphereTestQuestion]:
        """è·å–KubeSphereæµ‹è¯•é—®é¢˜"""
        if question_set == "quick":
            return get_quick_evaluation_questions()
        elif question_set == "full":
            return KubeSphereQuestionBank.get_all_questions()
        elif question_set.startswith("category:"):
            category = question_set.split(":", 1)[1]
            return KubeSphereQuestionBank.get_questions_by_category(category)
        elif question_set.startswith("difficulty:"):
            difficulty = question_set.split(":", 1)[1]
            return KubeSphereQuestionBank.get_questions_by_difficulty(difficulty)
        else:
            logger.warning(f"æœªçŸ¥é—®é¢˜é›†ç±»å‹: {question_set}ï¼Œä½¿ç”¨å¿«é€Ÿæµ‹è¯•é›†")
            return get_quick_evaluation_questions()

    def _generate_kubesphere_report(
        self,
        detailed_results: List[ComparisonResult],
        overall_evaluation: OverallEvaluation,
        timestamp: str
    ) -> str:
        """ç”ŸæˆKubeSphereä¸“é—¨è¯„ä¼°æŠ¥å‘Š"""
        report_file = os.path.join(
            self.output_dir, f"kubesphere_report_{timestamp}.md"
        )

        try:
            with open(report_file, 'w', encoding='utf-8') as f:
                f.write("# KubeSphere RAGç³»ç»Ÿè¯„ä¼°æŠ¥å‘Š\n\n")
                f.write(f"**è¯„ä¼°æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")

                # æ‰§è¡Œæ‘˜è¦
                f.write("## ğŸ“Š æ‰§è¡Œæ‘˜è¦\n\n")
                f.write(f"- **æ€»é—®é¢˜æ•°**: {overall_evaluation.total_questions}\n")
                f.write(f"- **RAGè·èƒœ**: {overall_evaluation.rag_wins} æ¬¡ ({overall_evaluation.rag_wins/overall_evaluation.total_questions:.1%})\n")
                f.write(f"- **åŸºçº¿è·èƒœ**: {overall_evaluation.baseline_wins} æ¬¡ ({overall_evaluation.baseline_wins/overall_evaluation.total_questions:.1%})\n")
                f.write(f"- **å¹³å±€**: {overall_evaluation.ties} æ¬¡ ({overall_evaluation.ties/overall_evaluation.total_questions:.1%})\n")
                f.write(f"- **å¹³å‡æ”¹è¿›**: {overall_evaluation.avg_improvement:.1%}\n")
                f.write(f"- **ç»“è®º**: {overall_evaluation.performance_summary['overall_conclusion']}\n\n")

                # è´¨é‡åˆ†æ
                f.write("## ğŸ“ˆ è´¨é‡åˆ†æ\n\n")
                f.write("### æ•´ä½“è´¨é‡åˆ†æ•°\n\n")
                f.write(f"| æŒ‡æ ‡ | RAGç³»ç»Ÿ | åŸºçº¿æ¨¡å‹ | æ”¹è¿›å¹…åº¦ |\n")
                f.write(f"|------|---------|----------|----------|\n")
                f.write(f"| å¹³å‡åˆ†æ•° | {overall_evaluation.avg_rag_score:.3f} | {overall_evaluation.avg_baseline_score:.3f} | {overall_evaluation.avg_improvement:.1%} |\n\n")

                # åˆ†ç±»åˆ«æ€§èƒ½
                f.write("### åˆ†ç±»åˆ«æ€§èƒ½è¡¨ç°\n\n")
                f.write("| ç±»åˆ« | RAGèƒœç‡ | å¹³å‡RAGåˆ†æ•° | å¹³å‡åŸºçº¿åˆ†æ•° | æ”¹è¿›å¹…åº¦ | é—®é¢˜æ•° |\n")
                f.write("|------|---------|-------------|-------------|----------|--------|\n")
                for category, perf in overall_evaluation.category_performance.items():
                    f.write(f"| {category} | {perf['win_rate']:.1%} | {perf['avg_rag_score']:.3f} | {perf['avg_baseline_score']:.3f} | {perf['avg_improvement']:.1%} | {perf['total_questions']} |\n")
                f.write("\n")

                # å…³é”®æ´å¯Ÿ
                f.write("## ğŸ” å…³é”®æ´å¯Ÿ\n\n")
                best_category = overall_evaluation.performance_summary['best_category']
                worst_category = overall_evaluation.performance_summary['worst_category']
                f.write(f"- **æœ€ä½³è¡¨ç°ç±»åˆ«**: {best_category}\n")
                f.write(f"- **éœ€è¦æ”¹è¿›ç±»åˆ«**: {worst_category}\n")
                f.write(f"- **RAGèƒœç‡**: {overall_evaluation.performance_summary['rag_win_rate']:.1%}\n")
                f.write(f"- **å¹³å‡æ”¹è¿›ç™¾åˆ†æ¯”**: {overall_evaluation.performance_summary['avg_improvement_percentage']:.1f}%\n\n")

                # è¯¦ç»†ç»“æœ
                f.write("## ğŸ“ è¯¦ç»†ç»“æœ\n\n")
                for i, result in enumerate(detailed_results, 1):
                    f.write(f"### é—®é¢˜ {i}: {result.question.question}\n\n")
                    f.write(f"**ç±»åˆ«**: {result.question.category} | **éš¾åº¦**: {result.question.difficulty}\n\n")
                    f.write(f"**è·èƒœè€…**: {result.winner} | **æ”¹è¿›åˆ†æ•°**: {result.improvement_score:.3f}\n\n")

                    f.write("#### RAGå›ç­”\n")
                    f.write(f"**è´¨é‡åˆ†æ•°**: {result.rag_quality.overall_score:.3f}\n")
                    f.write(f"**å›ç­”**: {result.rag_answer}\n\n")

                    f.write("#### åŸºçº¿å›ç­”\n")
                    f.write(f"**è´¨é‡åˆ†æ•°**: {result.baseline_quality.overall_score:.3f}\n")
                    f.write(f"**å›ç­”**: {result.baseline_answer}\n\n")

                    f.write("---\n\n")

                # æŠ€æœ¯é™„å½•
                f.write("## ğŸ”§ æŠ€æœ¯é™„å½•\n\n")
                f.write("### è¯„ä¼°æŒ‡æ ‡è¯´æ˜\n\n")
                f.write("- **å¿ å®åº¦**: ç­”æ¡ˆæ˜¯å¦åŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹\n")
                f.write("- **ä¸€è‡´æ€§**: ç­”æ¡ˆå†…åœ¨é€»è¾‘æ˜¯å¦ä¸€è‡´\n")
                f.write("- **å®Œæ•´æ€§**: æ˜¯å¦å……åˆ†å›ç­”äº†é—®é¢˜\n")
                f.write("- **æ•´ä½“åˆ†æ•°**: å¿ å®åº¦Ã—0.4 + ä¸€è‡´æ€§Ã—0.3 + å®Œæ•´æ€§Ã—0.3\n\n")

                f.write("### è¯„ä¼°ç¯å¢ƒ\n\n")
                f.write(f"- **RAGæ¨¡å‹**: {self.rag_generator.llm_client.model if hasattr(self.rag_generator.llm_client, 'model') else 'Unknown'}\n")
                f.write(f"- **æ£€ç´¢æ–¹æ³•**: æ··åˆæ£€ç´¢ï¼ˆå¯†é›†å‘é‡ + ç¨€ç–å…³é”®è¯ï¼‰\n")
                f.write(f"- **çŸ¥è¯†åº“**: KubeSphereç›¸å…³æ–‡æ¡£\n")
                f.write(f"- **è¯„ä¼°æ—¶é—´**: {timestamp}\n\n")

            logger.info(f"KubeSphereè¯„ä¼°æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
            return report_file

        except Exception as e:
            logger.error(f"ç”ŸæˆKubeSphereæŠ¥å‘Šå¤±è´¥: {str(e)}")
            return ""

    def _generate_custom_report(
        self,
        detailed_results: List[ComparisonResult],
        overall_evaluation: OverallEvaluation,
        timestamp: str
    ) -> str:
        """ç”Ÿæˆè‡ªå®šä¹‰è¯„ä¼°æŠ¥å‘Š"""
        report_file = os.path.join(
            self.output_dir, f"custom_report_{timestamp}.md"
        )

        try:
            with open(report_file, 'w', encoding='utf-8') as f:
                f.write("# RAGç³»ç»Ÿè‡ªå®šä¹‰è¯„ä¼°æŠ¥å‘Š\n\n")
                f.write(f"**è¯„ä¼°æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")

                # åŸºæœ¬ä¿¡æ¯ä¸æ‘˜è¦éƒ¨åˆ†ï¼ˆç±»ä¼¼KubeSphereæŠ¥å‘Šï¼‰
                self._write_basic_summary(f, overall_evaluation)

                # ç®€åŒ–çš„è¯¦ç»†ç»“æœ
                f.write("## ğŸ“ è¯„ä¼°ç»“æœè¯¦æƒ…\n\n")
                for i, result in enumerate(detailed_results, 1):
                    f.write(f"### é—®é¢˜ {i}\n\n")
                    f.write(f"**é—®é¢˜**: {result.question.question}\n\n")
                    f.write(f"**è·èƒœè€…**: {result.winner} | **æ”¹è¿›åˆ†æ•°**: {result.improvement_score:.3f}\n\n")
                    f.write("---\n\n")

            logger.info(f"è‡ªå®šä¹‰è¯„ä¼°æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
            return report_file

        except Exception as e:
            logger.error(f"ç”Ÿæˆè‡ªå®šä¹‰æŠ¥å‘Šå¤±è´¥: {str(e)}")
            return ""

    def _write_basic_summary(self, f, overall_evaluation: OverallEvaluation):
        """å†™å…¥åŸºæœ¬æ‘˜è¦ä¿¡æ¯"""
        f.write("## ğŸ“Š è¯„ä¼°æ‘˜è¦\n\n")
        f.write(f"- **æ€»é—®é¢˜æ•°**: {overall_evaluation.total_questions}\n")
        f.write(f"- **RAGè·èƒœ**: {overall_evaluation.rag_wins} æ¬¡\n")
        f.write(f"- **åŸºçº¿è·èƒœ**: {overall_evaluation.baseline_wins} æ¬¡\n")
        f.write(f"- **å¹³å±€**: {overall_evaluation.ties} æ¬¡\n")
        f.write(f"- **RAGèƒœç‡**: {overall_evaluation.rag_wins/overall_evaluation.total_questions:.1%}\n")
        f.write(f"- **å¹³å‡æ”¹è¿›**: {overall_evaluation.avg_improvement:.1%}\n")
        f.write(f"- **ç»“è®º**: {overall_evaluation.performance_summary['overall_conclusion']}\n\n")

    def quick_test(self, num_questions: int = 5) -> Dict[str, Any]:
        """å¿«é€Ÿæµ‹è¯•åŠŸèƒ½"""
        logger.info(f"å¼€å§‹å¿«é€Ÿæµ‹è¯•ï¼Œä½¿ç”¨ {num_questions} ä¸ªé—®é¢˜")

        try:
            # è·å–å¿«é€Ÿæµ‹è¯•é—®é¢˜
            kubesphere_questions = get_quick_evaluation_questions()[:num_questions]

            # è½¬æ¢æ ¼å¼
            evaluation_questions = [
                EvaluationQuestion(
                    question=kq.question,
                    category=kq.category,
                    expected_knowledge=kq.expected_knowledge,
                    difficulty=kq.difficulty
                )
                for kq in kubesphere_questions
            ]

            # è¿è¡Œè¯„ä¼°ï¼ˆä¸ä¿å­˜ç»“æœï¼‰
            detailed_results, overall_evaluation = self.evaluator.evaluate_question_set(
                evaluation_questions, save_results=False
            )

            # è¿”å›ç®€åŒ–ç»“æœ
            return {
                "total_questions": len(detailed_results),
                "rag_wins": overall_evaluation.rag_wins,
                "baseline_wins": overall_evaluation.baseline_wins,
                "ties": overall_evaluation.ties,
                "rag_win_rate": overall_evaluation.rag_wins / len(detailed_results),
                "avg_improvement": overall_evaluation.avg_improvement,
                "conclusion": overall_evaluation.performance_summary['overall_conclusion'],
                "questions_tested": [r.question.question for r in detailed_results]
            }

        except Exception as e:
            logger.error(f"å¿«é€Ÿæµ‹è¯•å¤±è´¥: {str(e)}")
            return {"error": str(e)}

    def print_quick_summary(self, overall_evaluation: OverallEvaluation):
        """æ‰“å°å¿«é€Ÿæ‘˜è¦"""
        print(f"\n{'='*50}")
        print("RAG vs åŸºçº¿æ¨¡å‹è¯„ä¼°ç»“æœ")
        print(f"{'='*50}")
        print(f"æ€»é—®é¢˜æ•°: {overall_evaluation.total_questions}")
        print(f"RAGè·èƒœ: {overall_evaluation.rag_wins} æ¬¡ ({overall_evaluation.rag_wins/overall_evaluation.total_questions:.1%})")
        print(f"å¹³å‡æ”¹è¿›: {overall_evaluation.avg_improvement:.1%}")
        print(f"ç»“è®º: {overall_evaluation.performance_summary['overall_conclusion']}")
        print(f"{'='*50}\n")


# ä¾¿æ·å‡½æ•°
def run_quick_kubesphere_evaluation(
    rag_generator: RAGGenerator,
    retriever: HybridRetriever,
    num_questions: int = 5
) -> Dict[str, Any]:
    """è¿è¡Œå¿«é€ŸKubeSphereè¯„ä¼°

    Args:
        rag_generator: RAGç”Ÿæˆå™¨
        retriever: æ£€ç´¢å™¨
        num_questions: æµ‹è¯•é—®é¢˜æ•°é‡

    Returns:
        è¯„ä¼°ç»“æœæ‘˜è¦
    """
    runner = EvaluationRunner(rag_generator, retriever)
    return runner.quick_test(num_questions)


def run_full_kubesphere_evaluation(
    rag_generator: RAGGenerator,
    retriever: HybridRetriever,
    question_set: str = "quick"
) -> Tuple[List[ComparisonResult], OverallEvaluation, str]:
    """è¿è¡Œå®Œæ•´KubeSphereè¯„ä¼°

    Args:
        rag_generator: RAGç”Ÿæˆå™¨
        retriever: æ£€ç´¢å™¨
        question_set: é—®é¢˜é›†ç±»å‹

    Returns:
        (è¯¦ç»†ç»“æœ, æ•´ä½“è¯„ä¼°, æŠ¥å‘Šæ–‡ä»¶è·¯å¾„)
    """
    runner = EvaluationRunner(rag_generator, retriever)
    return runner.run_kubesphere_evaluation(question_set)


if __name__ == "__main__":
    # æ¼”ç¤ºç”¨æ³•
    print("RAGè¯„ä¼°è¿è¡Œå™¨æ¼”ç¤º")
    print("è¯·åœ¨å®é™…é¡¹ç›®ä¸­å¯¼å…¥å¹¶ä½¿ç”¨ç›¸åº”ç»„ä»¶")